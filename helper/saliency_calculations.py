import os
import torch
import torchvision
from torch import nn
from torchvision.transforms.functional import to_tensor
from tqdm import tqdm
from PIL import Image
import numpy as np

    
def compute_saliency_maps_v2(X, y, model, criterion):
    
    model.eval()
    X.requires_grad_()

    scores = model(X)
    loss = criterion(scores, y)
    loss.backward()

    saliency = X.grad.data.abs()
    
    return saliency
    
    

def save_image_and_saliency_v2(image, filename, folder, model, device, criterion):
    
    X_tensor = image.unsqueeze(0).to(device)
    y_tensor = torch.tensor([1], dtype=torch.float32).to(device).unsqueeze(-1)
    
    # Compute saliency
    saliency = compute_saliency_maps_v2(X_tensor, y_tensor, model, criterion)

    """
    # Save original image
    original_img_path = os.path.join(folder, f"Original_{filename}")
    img_array = X_tensor.squeeze().cpu().detach().numpy()
    img = Image.fromarray(np.uint8(img_array*255), 'L')
    img.save(original_img_path)
    """

    # Save saliency map
    saliency_img_path = os.path.join(folder, f"Saliency_{filename}")
    saliency_array = saliency.squeeze().cpu().detach().numpy()
    
    range_values = saliency_array.max() - saliency_array.min()
    saliency_normalized = (saliency_array - saliency_array.min()) / range_values * 255 if range_values != 0 else np.zeros_like(saliency_array)
    # saliency_normalized = (saliency_array - saliency_array.min()) / (saliency_array.max() - saliency_array.min()) * 255
    saliency_img = Image.fromarray(np.uint8(saliency_normalized), 'L')
    saliency_img.save(saliency_img_path)
    
    
    
def save_images_and_saliencies_v2(dataloader, folder, model, device, criterion):
    
    for (images, filenames) in tqdm(dataloader, desc="Making saliency maps..."):
        for img, file in zip(images, filenames):
            save_image_and_saliency_v2(img, file, folder, model, device, criterion)