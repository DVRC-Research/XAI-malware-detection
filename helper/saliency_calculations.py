import os
import torch
from tqdm import tqdm
from PIL import Image
import numpy as np

    
def compute_saliency_maps(X, y, model, criterion):
    
    model.eval()
    X.requires_grad_()

    scores = model(X)
    loss = criterion(scores, y)
    loss.backward()

    saliency = X.grad.data.abs()
    
    return saliency
    
    

def save_image_and_saliency(image, filename, folder, model, device, criterion):
    
    X_tensor = image.unsqueeze(0).to(device)
    y_tensor = torch.tensor([1], dtype=torch.float32).to(device).unsqueeze(-1)
    
    # Compute saliency
    saliency = compute_saliency_maps(X_tensor, y_tensor, model, criterion)

    # Normalize saliency map
    saliency_array = saliency.squeeze().cpu().detach().numpy()
    range_values = saliency_array.max() - saliency_array.min()
    saliency_normalized = (saliency_array - saliency_array.min()) / range_values * 255 if range_values != 0 else np.zeros_like(saliency_array)

    # Save saliency map
    saliency_img = Image.fromarray(np.uint8(saliency_normalized), 'L')
    saliency_img_path = os.path.join(folder, f"Saliency_{filename}")
    saliency_img.save(saliency_img_path)
    
    
    
def save_images_and_saliencies(dataloader, folder, model, device, criterion):
    
    for (images, filenames) in tqdm(dataloader, desc="Making saliency maps..."):
        for img, file in zip(images, filenames):
            save_image_and_saliency(img, file, folder, model, device, criterion)